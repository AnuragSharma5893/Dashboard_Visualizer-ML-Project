{"nbformat": 4, "nbformat_minor": 0, "metadata": {}, "cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "source": ["get_ipython().run_line_magic('pip', 'install comet_ml torch datasets transformer scikit-learn')"], "outputs": []}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "source": ["get_ipython().run_line_magic('pip', 'install comet_ml torch datasets transformers scikit-learn')"], "outputs": []}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "source": ["Initialize Comet"], "outputs": []}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "source": ["Initialize Comet"], "outputs": []}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "source": ["import comet_ml\n", "\n", "comet_ml.init(project_name = \"imdb-distrilbart\")"], "outputs": []}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "source": ["PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\"\n", "SEED = 20"], "outputs": []}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "source": ["from transformers import AutoTokenizer, Tranier, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "raw_datasets = load_dataset(\"imdb\")"], "outputs": []}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "source": ["get_ipython().system('pip install Tranier')"], "outputs": []}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "source": ["from transformers import AutoTokenizer, Tranier, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "raw_datasets = load_dataset()"], "outputs": []}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "source": ["get_ipython().system('pip install transformers ')"], "outputs": []}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "source": ["from transformers import AutoTokenizer, Trainer, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "raw_datasets = load_datasets(\"imdb\")"], "outputs": []}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "source": ["from transformers import AutoTokenizer, Trainer, TrainingArguments\n", "from datasets import load_dataset\n", "\n", "raw_datasets = load_dataset(\"imdb\")"], "outputs": []}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "source": ["tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"], "outputs": []}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "source": ["def tokenize_function(examples):\n", "  return tokenizer(examples[\"text\"],padding=\"max_length\", truncation=True)\n", "\n", "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)  "], "outputs": []}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "source": ["from transformers import DataCollactorWithPadding \n", "\n", "data_collator= data_CollatorWithPadding(tokenizer=tokenizer)"], "outputs": []}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "source": ["get_ipython().system('pip install  transformers ')"], "outputs": []}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "source": ["from transformers import DataCollactorWithPadding \n", "\n", "data_collator= DataCollatorWithPadding(tokenizer=tokenizer)"], "outputs": []}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "source": ["from transformers import DataCollatorWithPadding \n", "\n", "data_collator= DataCollatorWithPadding(tokenizer=tokenizer)"], "outputs": []}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "source": ["train_dataset = tokenized_datasets[\"train\"].shuffle(SEED).select(range(200))\n", "eval_dataset = tokenized_datasets[\"test\"].shuffle(SEED).select"], "outputs": []}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "source": ["train_dataset = tokenized_datasets[\"train\"].shuffle(SEED).select(range(200))\n", "eval_dataset = tokenized_datasets[\"test\"].shuffle(SEED).select(range(200))"], "outputs": []}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "source": ["train_dataset = tokenized_datasets[\"train\"].shuffle(SEED).select(range(200))\n", "eval_dataset = tokenized_datasets[\"test\"].shuffle(SEED).select()"], "outputs": []}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "source": ["train_dataset = tokenized_datasets[\"train\"].shuffle(SEED).select(range(200))\n", "eval_dataset = tokenized_datasets[\"test\"].shuffle(SEED).select(range(200))"], "outputs": []}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "source": ["from transformers import AutoModelForSequenceClassification\n", "\n", "model = AutoModelForSequenceClassification.from_pretrained(\n", "    PRE_TRAINED_MODEL_NAME, num_labels=2\n", ")"], "outputs": []}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "source": ["from scikit.metrics import accuracy_score , precision_recall_fscore_support \n", "\n", "def get_example(index):\n", "  return eval_dataset[index][\"text\"]\n", "\n", "def compute_metrics(pred):\n", "  experiment = comet_ml.get_global_experiment()\n", "  \n", "  labels = pred.label_ids\n", "  preds = pred.predictions.argmax(-1)\n", "  precision,recall, f1, _ = precision_recall_fscore_support(\n", "      labels, preds, average=\"macro\"\n", "  )\n", "  acc = accuracy "], "outputs": []}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "source": ["from six import iteritems\n", "from sklearn.metrics import accuracy_score , precision_recall_fscore_support \n", "\n", "def get_example(index):\n", "  return eval_dataset[index][\"text\"]\n", "\n", "def compute_metrics(pred):\n", "  experiment = comet_ml.get_global_experiment()\n", "  \n", "  labels = pred.label_ids\n", "  preds = pred.predictions.argmax(-1)\n", "  precision,recall, f1, _ = precision_recall_fscore_support(\n", "      labels, preds, average=\"macro\"\n", "  )\n", "  acc = accuracy_score(labels,preds)\n", "\n", "  if experiment:\n", "    epoch = int(experiment.curr_epoch) if experiment.curr_epoch is not None else 0\n", "    experiment.set_epoch(epoch)\n", "    experiment.log_confusion_matrix(\n", "        y_true = labels,\n", "        y_predicted = preds,\n", "        file_name = f\"confusion-matrix-epoch-{epoch}.json\",\n", "        labels = [\"negative\",\"positive\"],\n", "        index_to_example_function = get_example\n", "    )\n", "    for i in range(20):\n", "      experiment.log_text(get_example(i), metadata={\"label\": labels[i],item()})\n", "\n", "    return {\"accuracy\":acc, \"f1\": f1, \"precision\": precision,\"recall\": recall}  "], "outputs": []}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "source": ["from six import iteritems\n", "from sklearn.metrics import accuracy_score , precision_recall_fscore_support \n", "\n", "def get_example(index):\n", "  return eval_dataset[index][\"text\"]\n", "\n", "def compute_metrics(pred):\n", "  experiment = comet_ml.get_global_experiment()\n", "  \n", "  labels = pred.label_ids\n", "  preds = pred.predictions.argmax(-1)\n", "  precision,recall, f1, _ = precision_recall_fscore_support(\n", "      labels, preds, average=\"macro\"\n", "  )\n", "  acc = accuracy_score(labels,preds)\n", "\n", "  if experiment:\n", "    epoch = int(experiment.curr_epoch) if experiment.curr_epoch is not None else 0\n", "    experiment.set_epoch(epoch)\n", "    experiment.log_confusion_matrix(\n", "        y_true = labels,\n", "        y_predicted = preds,\n", "        file_name = f\"confusion-matrix-epoch-{epoch}.json\",\n", "        labels = [\"negative\",\"positive\"],\n", "        index_to_example_function = get_example\n", "    )\n", "    for i in range(20):\n", "      experiment.log_text(get_example(i), metadata={\"label\": labels[i].item()})\n", "\n", "    return {\"accuracy\":acc, \"f1\": f1, \"precision\": precision,\"recall\": recall}  "], "outputs": []}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir= True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_steps=25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collactor\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir= True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_steps = 25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collactor\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir= True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_steps = 25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir= True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_step = 25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir= True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_steps=25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir=True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    eval_steps=25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir=True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    evaluation_steps=25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir=True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\"\n", "    evaluation_steps=20,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "source": ["get_ipython().run_line_magic('env', 'COMET_MODE=ONLINE')\n", "get_ipython().run_line_magic('env', 'COMET_LOG_ASSETS=TRUE')\n", "\n", "training_args = TrainingArguments(\n", "    seed=SEED,\n", "    output_dir=\"./results\",\n", "    overwrite_output_dir=True,\n", "    num_train_epochs=1,\n", "    do_train=True,\n", "    do_eval=True,\n", "    evaluation_strategy=\"steps\",\n", "    eval_steps=25,\n", "    save_strategy=\"steps\",\n", "    save_total_limit=10,\n", "    save_steps=25,\n", "    per_device_train_batch_size=8\n", ")\n", "\n", "trainer = Trainer (\n", "    model=model,\n", "    args= training_args,\n", "    train_dataset= train_dataset,\n", "    eval_dataset= eval_dataset,\n", "    compute_metrics= compute_metrics,\n", "    data_collator=data_collator\n", ")\n", "trainer.train()"], "outputs": []}]}